1. 선형 회귀 모델 (Linear Regression)
    1) 선형 회귀식을 활용한 모델
    2) 선형 회귀 모델을 훈련한다는 것은 훈련 데이터에 잘 맞는 모델 파라미터, 즉 회귀계수를 찾는 것임 
    3) 구현
        - 데이터 생성 --> 모델 훈련 --> 그래프로 확인 
        - 단순 선형 회귀식 : y = 2x + 5

2. 로지스틱 (Logistic) 회귀 모델            
    1) '회귀'라는 단어때문에 회귀 문제에 사용할 것 같지만, 분류 문제에 사용함
    2) 선형 회귀 모델을 응용해 분류에 적용한 모델임 
    3) 로지스틱 회귀는 시그모이드 함수를 활용해 타겟값에 포함될 확률을 예측함 
        - 시그모이드 함수는 0에서 1 사이 값을 갖음 
        - x가 0일때 0.5임 => 확률로 해석하면 타겟값에 포함될 확률이 구해짐 
            - 타깃값 예측 
            - 시그모이드 값(확률)이 0.5보다 작으면 0(음성),
                                  0.5이상이면 1(양성)이라고 예측함   
                - 로지스틱 회귀 타깃값 예측 
                    - 0 (p < 0.5)
                      1 (p >= 0.5)     

                    - 예측값            타깃값일 확률 (p)               타깃값(y)
                      산출식            p = sigmoid(x)      
                      예시                  0.1                          0 (음성)
                                            0.4                          0 (음성)
                                            0.6                          1 (양성)   
                                            0.9                          1 (양성)

3. 결정 트리 (decision tree)        
    1) 의사결정 나무        
    2) 분류와 회귀 문제에 모두 사용 가능한 모델임   
    3) 작동 원리
        - 데이터를 가장 잘 구분하는 조건을 정함                           
        - 조건을 기준으로 데이터를 두 범주로 나눔 
        - 나뉜 각 범주의 데이터를 잘 구분하는 조건을 다시 정함 
        - 조건에 따라 각 범주에 속한 데이터를 다시 분할함 
        - 이런 식으로 계속 분할해 최종 결정 값을 구함 
    
    4) 결정트리 분할 방식 
        - 결정 트리를 만들 때는 분할 조건이 중요함 
        - 조건에 따라 분할 후 만들어지는 트리 모양이 다르기 때문임 

    5) 불순도 (Impurity)    
        - 하나의 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지를 나타내는 정도
        - 노드 내 데이터의 불순도를 최소화하는 방향으로 분할해야함 
        - 한 범주에 데이터 한 종류만 있다면 불순도가 최소임 (순도가 최대),
          서로 다른 데이터가 같은 비율로 있다면 불순도가 최대임 (순도가 최소)

    6) 불순도를 측정하는 지표 
        - 엔트로피 (entropy)
            - '불확실한 정도'
                - 한 범주에 데이터가 한 종류만 있다면 엔트로피는 0임 
                - 서로 다른 데이터의 비율이 비등해질수록 엔트로피는 1에 가까워짐 
            - 엔트로피 값이 클수록 불순도가 높고, 작을수록 불순도도 낮음
        - 정보 이득 (information gain)
            - 1 - 엔트로피 
            - 결정 트리는 정보 이득을 최대화하는 방향(엔트로피를 최소화하는 방향)으로 노드를 분할함 

        - 지니 불순도 (gini impurity)          
            - 엔트로피와 비슷한 개념임 
            - 지니 불순도 값이 클수록 불순도도 높고, 작을수록 불순도도 낮음 
            - 지니 불순도가 낮아지는 방향으로 노드를 분할함 

    7) 결정 트리 구현 
        - 분류형 모델은 DecisionTreeClassifier,
          회귀형 모델은 DecisionTreeRegressor 임.  

        - DecisionTreeClassifier의 파라미터 
            - criterion : 분할 시 사용할 불순도 측정 지표 
                            기본값 = 'gini'
            - max_depth : 트리의 최대 깊이.       
                          max_depth를 규정하지 않으면 모든 말단 노드의 불순도가 0에 가까워질 때까지 
                          트리 깊이가 깊어짐.
                          기본값 = None
            - min_samples_split : 노드 분할을 위한 최소 데이터 개수.                                                                    
                                  노드 내 데이터 개수가 이 값보다 작으면 더 이상 분할하지 않음.
                                  정수형으로 전달하면 최소 데이터 개수를 의미함 
                                  기본값 = 2 
            - min_samples_leaf : 말단 노드가 되기 위한 최소 데이터 개수      
                                 기본값 = 1 
            - max_features :  분할에 사용할 피처 개수 
                              기본값 = None                                                                

4. 앙상블 학습 (ensemble learning)
    1) 다양한 모델이 내린 예측 결과를 결합하는 기법을 앙상블 학습이라고 함 
    2) 앙상블 학습을 활용하면 대체로 예측 성능이 좋아짐
    3) 앙상블 학습 유형
        - 보팅, 배깅, 부스팅   

5. 보팅
    1) 서로 다른 모델로 예측한 결과가 여럿 있음 
    2) 개별 결과를 종합해 최종 결과를 결졍하는 방식을 보팅(voting)이라고 함.
    3) 하드 보팅 (hard voting)
        - '다수결 투표' 방식으로 최종 예측값을 정함 
        - 가장 많은 표를 얻은 개별 예측값을 최종 예측값으로 정함 (최빈값)
    4) 소프트 보팅 (soft voting)
        - 개별 예측 확률들의 평균을 최종 예측확률로 정하는 방식    
        - 예

                        모델1   모델2  모델3   모델4    모델5   
         개별예측확률     0.5     0.7    0.9     0.6     0.8
        최종 예측확률                 0.7     

6. 배깅 (Bagging)
    1) 개별 모델로 예측한 결과를 결합해 최종 예측을 정하는 기법임        
    2) 개별 모델이 서로 다른 무작위 샘플링 데이터를 활용함 
        => 훈련된 개별 모델로 결과 예측됨 
        => 개별 모델의 수만큼 작업을 반복함 
        => 각 모델이 예측한 값들을 보팅하여 최종 예측값을 구함 
    3) 배깅은 원리가 간단하면서도 성능을 높일 수 있는 효과적인 기법임.
    4) 배깅 기법을 활용한 대표적인 모델이 랜덤 포레스트임.    

7. 부스팅 (boosting)
    1) 가중치를 활용해서 분류 성능이 약한 모델을 강하게 만드는 기법임 
    2) 배깅은 결정 트리1과 결정 트리2가 서로 독립적으로 결과를 예측한 다음
       보팅하여 최종 결과를 도출해 냄 
    3) 부스팅에서는 모델 간 협력이 이루어짐 
        - 이전 모델이 잘못 예측한 데이터에 가중치를 부여함 
        - 다음 모델은 이전 모델이 잘못 예측한 데이터(가중치가 부여된 데이터)에 더 집중해 훈련함    
            - 가중치가 부여된 데이터를 그만큼 더 중요하다고 판단해 더 잘 분류하려고 함 
            - 이런 단계를 반복하면 모델 성능이 점차 향상됨 
    4) 부스팅 절차 
        - +와 -로 구성된 원본 데이터셋 준비 
        - 처음에는 모든 데이터에 동일한 가중치를 줌 
        - 분류 모델1로 +와 -를 분류함 
        - 분류 모델1이 잘못 분류한 데이터(동그라미 표시된 데이터)에 더 높은 가중치를 부여함 
        - 분류 모델2는 가중치가 부여된 데이터에 집중해 데이터를 분류함 
        - 분류 모델2가 잘못 분류한 데이터에 더욱 높은 가중치를 부여함 
        - 분류 모델3은 이전 단계에서 가중치가 부여된 데이터에 집중해 데이터를 분류함 

        - 분류 모델 1,2,3을 결합하면 +와 -를 제대로 구분하는 모델을 만들수 있음    
    5) 부스팅 기법을 활용한 대표적인 모델
        - XGBoost, LightGBM     